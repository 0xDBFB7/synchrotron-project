## examples/xafs/doc_deconv4.lar
# while inotifywait -e close_write *.lar ../../larch/xafs/*.py; do larch --exec doc_deconv4.lar; done

import matplotlib.pyplot as plt
from numpy import genfromtxt
import scipy
#enguage digitizer
input_data = genfromtxt('./resolving_power/ZnS_theoretical_0_1eV_broadening.csv', delimiter=',', skip_header=1) # there's room for improvement on both traces
# print(np.shape(input_data))
energies = input_data[:,0]
# energies = np.concatenate((np.array([161, 161.5, 162]), energies))
theory = input_data[:,1]
experiment = input_data[:,2]

experiment -= experiment[0]

series = np.linspace(energies[0], energies[-1], 1000)
experiment = scipy.interpolate.interp1d(energies, experiment, kind='cubic', fill_value='extrapolate')(series)
theory = scipy.interpolate.interp1d(energies, theory, kind='cubic', fill_value='extrapolate')(series)
energies = series

# the dataset was padded with a tapered ramp.
# surprisingly dramatic example.
# this is a worst-case scenario as regards noise because these were manually digitized from graphs.
# while the output is reasonable, the convergence properties are very different, suggesting a serious mistake in implementation
# the stock deconvolution algorithm flatly refused to run for reasons which have not yet been determined.
# The first few indices of the deconvolved array are invalid.
# LR provides an unphysical shift.
# possible throughput advantages for exploratory research.

# because of the free parameters in LR, it was possible to  

# the Fister paper describes validation 


data = group(energy=energies,mu=experiment)

pre_edge(data, e0=162, pre1=162, pre2=162)

autobk(data, rbkg=1.1, kweight=2)
xftf(data, kmin=2, kmax=17, dk=5, kwindow='kaiser', kweight=2)


# data.mu = 


# data.mu = smooth(data.energy, data.mu, sigma=0.1)
# data.norm = smooth(data.energy, data.norm, sigma=0.1)

data.mu -= data.mu[0]
data.norm -= data.norm[0]

data.mu /= np.max(data.mu)

data.norm = data.mu

# plot(data.energy, data.norm, label='Original',  win=1)
sps = 2
plt.style.use('grayscale')
plt.subplot(sps, 1, 1)
plt.title("Experimental input spectrum")
plt.plot(data.energy, data.norm)
# plt.xlabel("E (eV)")
plt.ylabel("Normalized arbitrary")


plt.subplot(sps, 1, 2)
plt.title("Deconvolution and theory lines")
theory -= theory[0]
theory /= np.max(theory)
plt.plot(data.energy[10:], theory[10:]+0.4, label='Theory', linestyle='dashed')



# core_width('Zn', edge='L3')
xas_iterative_deconvolve(data, esigma=0.5, eshift=-0.3, regularization_filter_width=0.005, max_iterations=300, grid_spacing=0.05)
# xas_iterative_deconvolve(data, esigma=0.4, eshift=0, regularization_filter_width=0.005, max_iterations=100, grid_spacing=0.005)

data.deconv /= np.max(data.deconv)
plt.plot(data.energy[10:], data.deconv[10:], label='Richardson-Lucy')
# plt.subplot(sps, 1, 3)

# plot(np.arange(len(data.convergence)), log(data.convergence), label='Convergence',  win=2)

xas_deconvolve(data, esigma=0.5, smooth=True)
# plt.subplot(sps, 1, 4)
data.deconv /= np.max(data.deconv)
plt.plot(data.energy[10:], data.deconv[10:], label='Inverse filtering')
plt.xlabel("E (eV)")
plt.ylabel("Normalized arbitrary")
# xas_deconvolve(data, esigma=0.5, smooth=False)
# with smooth=false, this doesn't work at all.

# theory = smooth(data.energy, theory, sigma=1.0) # representing figure 5 

plt.legend()
plt.show()
input()
## end examples/xafs/doc_deconv3.lar
